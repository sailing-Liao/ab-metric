{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167335b3",
   "metadata": {},
   "source": [
    "亲和力预测  \n",
    "即抗体与抗原的亲和力越高，则表明二者的结合越强"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce33d06",
   "metadata": {},
   "source": [
    "该部分使用csv或者FASTA格式的文件对亲和力进行预测  \n",
    "模型训练所给定的文件如下（以抗体的重链为例）  \n",
    "而抗原只需要给定对应的抗原序列即可"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff0df4",
   "metadata": {},
   "source": [
    "| heavy    | 重链序列，完整的重链氨基酸序列信息。                                    |\n",
    "| -------- | ----------------------------------------------------- |\n",
    "| cdr1     | 重链的互补决定区 1（Complementary Determining Region 1）的氨基酸序列。 |\n",
    "| cdr2     | 重链的互补决定区 2（Complementary Determining Region 2）的氨基酸序列。 |\n",
    "| cdr3     | 重链的互补决定区 3（Complementary Determining Region 3）的氨基酸序列。 |\n",
    "| affinity | 抗体与抗原之间的结合亲和力，通常以某种单位（如 pM 或 nM）表示。                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489780aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from base import BaseModel\n",
    "\n",
    "class BERTBinding(BaseModel):\n",
    "    def __init__(self, ReceptorBert_dir, emb_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.ReceptorBert = BertModel.from_pretrained(ReceptorBert_dir)\n",
    "        self.binding_predict = nn.Sequential(\n",
    "            nn.Linear(in_features=emb_dim, out_features=emb_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=dropout),\n",
    "\n",
    "            nn.Linear(in_features=emb_dim, out_features=1)\n",
    "        )\n",
    "        # self.binding_predict = nn.Sequential(\n",
    "        #     nn.Linear(in_features=32, out_features=32*2),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(in_features=32*2, out_features=32*4),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(in_features=32*4, out_features=32*2),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(in_features=32*2, out_features=32),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(in_features=32, out_features=1)\n",
    "        # )\n",
    "\n",
    "\n",
    "    def forward(self, epitope, receptor):\n",
    "        # shape: [batch_size, seq_length, emb_dim]\n",
    "        receptor_encoded = self.ReceptorBert(**receptor).last_hidden_state\n",
    "\n",
    "        '''\n",
    "        Using the cls (classification) token as the input to get the score which is borrowed\n",
    "        from huggingface NextSentencePrediciton implementation\n",
    "        https://github.com/huggingface/transformers/issues/7540\n",
    "        https://huggingface.co/transformers/v2.0.0/_modules/transformers/modeling_bert.html\n",
    "        '''\n",
    "        # shape: [batch_size, emb_dim]\n",
    "        receptor_cls = receptor_encoded[:, 0, :]\n",
    "        # receptor_cls = torch.squeeze(torch.sum(receptor_encoded, dim=1))\n",
    "        output = self.binding_predict(receptor_cls)\n",
    "        # output = self.binding_predict(receptor['input_ids'].type(torch.float))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6787331",
   "metadata": {},
   "source": [
    "还有基于ACE/SPR数据训练的模型函数，但所需输入为重链、轻链以及抗原序列，FASTA和txt格式应均可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed280e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "affinity_model = AutoModelForSequenceClassification.from_pretrained(\"Absci/affinity_predictor\")\n",
    "\n",
    "def predict_affinity(heavy_seq, light_seq, antigen_seq):\n",
    "    inputs = tokenizer(heavy_seq, light_seq, antigen_seq, return_tensors=\"pt\")\n",
    "    return affinity_model(**inputs).logits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
